{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwGIp9wZ27o+68N+IEEbhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubuntumel/AI_Colab_Projects/blob/main/Spam_Email_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3iz55c_wXFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2ba4fb-3623-416a-cdfb-4b6193245117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total rows loaded: 30\n",
            "Train size: 20, Test size: 10\n",
            "\n",
            "Priors calculation\n",
            "{'ham': 0.55, 'spam': 0.45}\n",
            "\n",
            "Test set:\n",
            "\n",
            "Test #1\n",
            "Sentence: Tell where you reached\n",
            "log P(sentence|ham)  = -8.197\n",
            "log P(sentence|spam) = -9.770\n",
            "P(ham|sentence)  = 0.855\n",
            "P(spam|sentence) = 0.145\n",
            "Predicted class: ham | True: ham\n",
            "\n",
            "Test #2\n",
            "Sentence: Your gonna have to pick up a burger for yourself on your way home\n",
            "log P(sentence|ham)  = -48.204\n",
            "log P(sentence|spam) = -47.537\n",
            "P(ham|sentence)  = 0.385\n",
            "P(spam|sentence) = 0.615\n",
            "Predicted class: spam | True: ham\n",
            "\n",
            "Test #3\n",
            "Sentence: As a valued customer I am pleased to advise you that for your recent review you are awarded a Bonus Prize\n",
            "log P(sentence|ham)  = -69.965\n",
            "log P(sentence|spam) = -70.084\n",
            "P(ham|sentence)  = 0.579\n",
            "P(spam|sentence) = 0.421\n",
            "Predicted class: ham | True: spam\n",
            "\n",
            "Test #4\n",
            "Sentence: Urgent you are awarded a complimentary trip to EuroDisinc To claim text immediately\n",
            "log P(sentence|ham)  = -38.621\n",
            "log P(sentence|spam) = -34.178\n",
            "P(ham|sentence)  = 0.014\n",
            "P(spam|sentence) = 0.986\n",
            "Predicted class: spam | True: spam\n",
            "\n",
            "Test #5\n",
            "Sentence: Finished class where are you\n",
            "log P(sentence|ham)  = -8.197\n",
            "log P(sentence|spam) = -9.077\n",
            "P(ham|sentence)  = 0.747\n",
            "P(spam|sentence) = 0.253\n",
            "Predicted class: ham | True: ham\n",
            "\n",
            "Test #6\n",
            "Sentence: where are you how did you perform\n",
            "log P(sentence|ham)  = -19.799\n",
            "log P(sentence|spam) = -24.627\n",
            "P(ham|sentence)  = 0.993\n",
            "P(spam|sentence) = 0.007\n",
            "Predicted class: ham | True: ham\n",
            "\n",
            "Test #7\n",
            "Sentence: you can call me now\n",
            "log P(sentence|ham)  = -23.265\n",
            "log P(sentence|spam) = -24.067\n",
            "P(ham|sentence)  = 0.732\n",
            "P(spam|sentence) = 0.268\n",
            "Predicted class: ham | True: ham\n",
            "\n",
            "Test #8\n",
            "Sentence: I am waiting Call me once you are free\n",
            "log P(sentence|ham)  = -31.057\n",
            "log P(sentence|spam) = -34.425\n",
            "P(ham|sentence)  = 0.973\n",
            "P(spam|sentence) = 0.027\n",
            "Predicted class: ham | True: ham\n",
            "\n",
            "Test #9\n",
            "Sentence: I am on the way to homei\n",
            "log P(sentence|ham)  = -22.166\n",
            "log P(sentence|spam) = -24.560\n",
            "P(ham|sentence)  = 0.930\n",
            "P(spam|sentence) = 0.070\n",
            "Predicted class: ham | True: ham\n",
            "\n",
            "Test #10\n",
            "Sentence: Please call our customer service representative between 10am-9pm as you have WON a guaranteed cash prize\n",
            "log P(sentence|ham)  = -55.768\n",
            "log P(sentence|spam) = -51.074\n",
            "P(ham|sentence)  = 0.011\n",
            "P(spam|sentence) = 0.989\n",
            "Predicted class: spam | True: spam\n",
            "\n",
            "Accuracy on Test Set\n",
            "8 / 10 = 0.800\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import math\n",
        "import re\n",
        "import shutil\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup persistence with Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    PERSIST_DIR = Path('/content/drive/MyDrive/colab_data')\n",
        "else:\n",
        "    PERSIST_DIR = Path.cwd() / \"colab_data\"\n",
        "\n",
        "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CSV_FILENAME = \"SpamDetection.csv\"\n",
        "CSV_Path = PERSIST_DIR / CSV_FILENAME\n",
        "\n",
        "# If CSV file is not in Drive yet but exists locally, copy it there\n",
        "local_candidate = Path.cwd() / CSV_FILENAME\n",
        "if not CSV_Path.exists() and local_candidate.exists():\n",
        "    try:\n",
        "        shutil.copy2(local_candidate, CSV_Path)\n",
        "        print(f\"Copied local '{local_candidate}' -> '{CSV_Path}' for persistence.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not copy local file to Drive: {e}\")\n",
        "\n",
        "# Load dataset\n",
        "csv_path = CSV_Path  # Always use Drive\n",
        "\n",
        "if not csv_path.exists():\n",
        "    print(f\"CSV file '{csv_path}' not found.\")\n",
        "    exit()\n",
        "\n",
        "rows = []\n",
        "with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for i, row in enumerate(reader):\n",
        "        if not row:\n",
        "            continue\n",
        "        if i == 0 and row[0].strip().lower() not in {\"ham\", \"spam\"}:\n",
        "            continue\n",
        "        label = row[0].strip().lower()\n",
        "        text = row[1].strip()\n",
        "        rows.append((label, text))\n",
        "\n",
        "print(f\"Total rows loaded: {len(rows)}\")\n",
        "if len(rows) < 30:\n",
        "    print(\"Expected ~30 rows.\")\n",
        "\n",
        "# Train test split\n",
        "# Use first 20 messages for training and the next 10 for testing.\n",
        "train = rows[:20]\n",
        "test = rows[20:30]\n",
        "\n",
        "print(f\"Train size: {len(train)}, Test size: {len(test)}\")\n",
        "\n",
        "# Tokenizer\n",
        "token_pattern = re.compile(r\"[a-zA-Z0-9']+\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return token_pattern.findall(text.lower())\n",
        "\n",
        "# Count word frequencies\n",
        "word_count = {\"ham\": Counter(), \"spam\": Counter()}\n",
        "doc_count = Counter()\n",
        "total_tokens = {\"ham\": 0, \"spam\": 0}\n",
        "\n",
        "for label, text in train:\n",
        "    doc_count[label] += 1\n",
        "    tokens = tokenize(text)\n",
        "    word_count[label].update(tokens)\n",
        "    total_tokens[label] += len(tokens)\n",
        "\n",
        "vocab = set(word_count[\"ham\"].keys()) | set(word_count[\"spam\"].keys())\n",
        "V = len(vocab)\n",
        "\n",
        "# Compute priors representing the probability of ham or spam in the training set.\n",
        "n_train = len(train)\n",
        "prior = {\n",
        "    \"ham\": doc_count[\"ham\"] / n_train if n_train else 0.0,\n",
        "    \"spam\": doc_count[\"spam\"] / n_train if n_train else 0.0\n",
        "}\n",
        "print(\"\\nPriors calculation\")\n",
        "print({k: round(v, 4) for k, v in prior.items()})\n",
        "\n",
        "# Likelihood and Posterior\n",
        "alpha = 1.0  # Laplace smoothing\n",
        "\n",
        "# log likelihood function of a message given a class\n",
        "def log_likelihood(tokens, label):\n",
        "    ll = 0.0\n",
        "    denom = total_tokens[label] + alpha * V\n",
        "    cwc = word_count[label]\n",
        "    for w, c in Counter(tokens).items():\n",
        "        if w not in vocab:\n",
        "            continue\n",
        "        pw = (cwc[w] + alpha) / denom\n",
        "        ll += c * math.log(pw)\n",
        "    return ll\n",
        "\n",
        "#log posterior function using Bayesâ€™ algorithm\n",
        "def log_posteriors(logp_ham, logp_spam):\n",
        "    m = max(logp_ham, logp_spam)\n",
        "    ph = math.exp(logp_ham - m)\n",
        "    ps = math.exp(logp_spam - m)\n",
        "    z = ph + ps\n",
        "    return ph / z, ps / z\n",
        "\n",
        "# Evaluate on test set\n",
        "# Compute log-likelihood for ham and spam\n",
        "# Add log prior\n",
        "# Convert to posterior probabilities\n",
        "# Predict the class with higher probability\n",
        "print(\"\\nTest set:\")\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for i, (label, text) in enumerate(test, start=1):\n",
        "    toks = tokenize(text)\n",
        "    ll_ham = log_likelihood(toks, \"ham\")\n",
        "    ll_spam = log_likelihood(toks, \"spam\")\n",
        "\n",
        "    # log prior + log likelihood\n",
        "    lp_ham = math.log(prior[\"ham\"] + 1e-15) + ll_ham\n",
        "    lp_spam = math.log(prior[\"spam\"] + 1e-15) + ll_spam\n",
        "\n",
        "    # posterior probabilities\n",
        "    p_ham, p_spam = log_posteriors(lp_ham, lp_spam)\n",
        "\n",
        "    # predicted class = whichever has higher posterior\n",
        "    pred = \"spam\" if p_spam > p_ham else \"ham\"\n",
        "\n",
        "    y_true.append(label)\n",
        "    y_pred.append(pred)\n",
        "\n",
        "    # print results for this test message\n",
        "    print(f\"\\nTest #{i}\")\n",
        "    print(f\"Sentence: {text}\")\n",
        "    print(f\"log P(sentence|ham)  = {ll_ham:.3f}\")\n",
        "    print(f\"log P(sentence|spam) = {ll_spam:.3f}\")\n",
        "    print(f\"P(ham|sentence)  = {p_ham:.3f}\")\n",
        "    print(f\"P(spam|sentence) = {p_spam:.3f}\")\n",
        "    print(f\"Predicted class: {pred} | True: {label}\")\n",
        "\n",
        "# Accuracy test = (correct predictions) / (total test messages)\n",
        "correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n",
        "acc = correct / len(y_true) if y_true else 0.0\n",
        "print(\"\\nAccuracy on Test Set\")\n",
        "print(f\"{correct} / {len(y_true)} = {acc:.3f}\")\n",
        "\n"
      ]
    }
  ]
}